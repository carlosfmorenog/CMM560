{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CMM560 Topic 7 - Convolutional Neural Networks\n",
    "![My network is training!](https://www.dropbox.com/s/z4o984p7307gkhm/mynetistraining.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aims of the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learn the particularities of Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Apply CNNs to image repositories in easy ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources for the Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Deep Learning with Python**. Fran√ßois Chollet. November 2017, ISBN 978161729443. Manning.\n",
    "    * Very recommendable book, it was written by the author of `Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Easy vs Virtually Impossible](https://www.dropbox.com/s/yutzwr2okzrvop6/XKCD-easy_vs_virtuallyimpossible.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* For today's lecture (and for the lab as well) we will use the *Hello World!* of image datasets... **MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This dataset contains 70'000 images (60k for training and 10k for testing) of handwritten numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The task is to recognise digits from 0 to 9 in $28 \\times 28$ images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This dataset can be obtained ether by importing it through `Tensorflow` or `Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Installing Tensorflow and Keras if not installed already\n",
    "!pip install tensorflow==2.11.0\n",
    "!pip install keras==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import Keras with Tensorflow backend and download the dataset\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Notice that contrary to my recommendation, this dataset is stored as a 3D matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This means that the dataset has 60k train/10k test rows, each one with a $28 \\times 28$ image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is for us to visualise the samples better (afterwards you will see that images need to be flattened to be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "The number is: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21a64ca41d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = 7\n",
    "\n",
    "print(X_train[sample].shape)\n",
    "print('The number is: '+str(Y_train[sample]))\n",
    "plt.imshow(X_train[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why do CNNs work better than fully connected networks for images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The convolutional operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Two types of layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `Dense` layers learn global patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `Conv` learn local patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 1. Local patterns learned from an image](https://www.dropbox.com/s/hq9d4g7pheyttfr/mnistlocal.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * CNNs not only classify, but also extract their own features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The patterns/features that CNNs learn are **translation invariant**\n",
    "    * Once it learns it, it can recognise it anywhere in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* They can learn **spatial hierarchies** of patterns\n",
    "    * Each layer learns different type of features\n",
    "        * First layer learns edges, second learns larger patterns, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 2. How a CNN learns features](https://www.dropbox.com/s/vc3rukz9excn8ev/featlearn.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For this diagram, humans learn top-bottom and CNNs bottom-top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do CNNs learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The images of the **MNIST** dataset are all $28 \\times 28$ pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The first **convolution** layer takes a $(28, 28, 1)$ image and outputs a **feature map** of size $(26, 26, 32)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is because it computes $32$ **filters** over the input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* That means that after the first layer, the network transforms the training images into 32 output channels, each containing a $26 \\times 26$ filter, which is a **response map**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 3. Visual representation of a response map](https://www.dropbox.com/s/qxvuudn5hde4myz/responsemap.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A response map is the response of a filter at different locations of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is how CNNs extract features; by applying filters over the images and finding responses to them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why is the response map $26 \\times 26$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why 32 filters?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "26x26 because of the border effect (or the striding), and 32 filters can be changed (enough for MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Parameters of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Size of the patches extracted:** Typically $3 \\times 3$ or $5 \\times 5$. In the example above, you can see $3 \\times 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Depth of the output feature map:** Number of filters. This can change, i.e. can start with a *depth* of 32 and finish with 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In `Keras`, you can import a `Conv2D` layer, to which you can pass these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Conv2D(output_depth,(window_height, window_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do Convolution Layers Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A convolution layer works by sliding the patches over the 3D input map, stopping at every location and extracting the 3D patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Each of these patches is flattened into a 1D vector of size (`output_depth`,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The process can be best illustrated using the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 4. How convolutional layers work](https://www.dropbox.com/s/v6cce6spsoppw01/transform.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Border Effect and Strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Notice that we started with a $(28, 28, 1)$ image and we ended with a $(26, 26, 32)$ **feature map**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Two main reasons for not having the same width & height:\n",
    "    * The **border effect** (which can be countered by applying `padding`)\n",
    "    * The use of **strides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Border Effect & Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Remember that a CNN uses a convolution layer that applies a filter for each position of the image, similar to sliding a window throughout the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* By nature, this sliding window cannot be centered exactly throughout the entire image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For instance, in a $5 \\times 5$ feature map, you could only center a $3 \\times 3$ window in 9 positions as shown in the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 5. Valid locations of 3x3 patches in 5x5 input](https://www.dropbox.com/s/cgkjmxzn9nxkmsm/window.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If you want to get an output with the same size as the input, you need to apply `padding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In short, this is like adding a *margin* to the image so that the filter can be centered in more positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In the next figure you can see how the $3 \\times 3$ filter can be located in 25 positions now, thus delivering a $5 \\times 5$ output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 6. How padding works](https://www.dropbox.com/s/6jesljdoxa5v5nq/padding.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In the `Conv2D` function in `Keras`, padding is enabled by setting the parameter `padding = 'valid'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Striding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When images are considerably large, you don't want the sliding window to stop at every position!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You may introduce a parameter called `stride` which allows your convolution window to skip positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A `stride=1` stops in every position, but for instance `stride=2` will make the filter to move with a step of 2, this skipping half of the positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recall the example presented before. Without considering padding, the $3 \\times 3$ filter will only stop at four positions of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 7. Stride of 2](https://www.dropbox.com/s/scuaobz41fxks8t/strideof2.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* This is rarely used in practice (people don't want to lose important features in between the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is more recommended to use `Max Pooling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You can add to your model a `Max Pooling` layer which halves your feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Max pooling extracts windows from the input much like a convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Why should we use it? Imagine a CNN with no max pooling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 8. Example of a CNN in Keras with no max pooling](https://www.dropbox.com/s/ryc28146axk23ej/nomaxpooling.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can print the summary of the model with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Fig 9. Summary of a CNN without max pooling](https://www.dropbox.com/s/oi3djvi7k85b86d/nomaxpoolingsummary.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Notice that the number of parameters grow drastically after each layer\n",
    "    * The final feature map (the one with $22 \\times 22 \\times 64 = 36'928$ parameters) has to be `flattened` and then a `Dense` layer has to be applied, resulting in 15 million parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* However, the model isn't learning a **hierarchy** of features! This means that as layers progress, the CNN would get smaller and smaller images and thus will be unable to learn the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You need the last layer to contain information about the whole image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Flatten Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Not to be confused with flattening an image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* After a convolution layer and before a `Dense` (i.e. fully connected layer), there is a `flatten` layer that transforms the matrix feature maps into vectors for the `Dense` layer to operate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 10. The flatten layer in a CNN](https://www.dropbox.com/s/zicquaubtbbuzdv/flattencnn.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Final example of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 11. Final example of a CNN](https://www.dropbox.com/s/4tlkt6myhu8fn4x/finalcnn.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I recommend you to also read [this source](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) if you have questions regarding any of the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other useful concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The following are not exclusive to CNNs, and are widely used in all NNs to further improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Reduces overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It is rare that you use a fully connected layer as this may extract features that are too related to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You implement a dropout percent per layer. This randomly disconnects layers from the previous layer into the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Machine learning has by default an iterative nature (recall Gradient Descent)\n",
    "    * The more you iterate things, the \"better\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* An epoch occurs when the entire dataset is passed forward and backward through the network once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* By passing the dataset multiple times, you can further reduce the loss and increase the training/validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The more epochs, the better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Not necessarily, as everything in life, bigger is not always better! CNNs can stall or overfit when trained a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Imagine training a network with the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You would need to pass 60k images in each epoch\n",
    "! This would take a while!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* You can set a `batch_size` to pass your data in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This may have an effect on your training results if the sequence of batches is not properly set\n",
    "    * i.e. if you only pass batches from the negative class first, and then the positive one, your classifier may get biased towards the first class before being able to learn from the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The ADAM Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A faster way to optimise [gradient descent](https://builtin.com/data-science/gradient-descent) compared to the more classical approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It may obtain worse results, but compensates with speed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Combination of RMSprop and Stochastic Gradient Descent with momentum. More info [here](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some common CNN problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 12. The most common mistake when using CNNs](https://www.dropbox.com/s/z4xrn157wxpoygk/cnn.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Translation but not rotation invariant, and they learn with the focus of the training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 13. The 2nd most common mistake when using CNNs](https://www.dropbox.com/s/d7ewodct05f1m13/dogpigloaf.gif?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "CNNs don't know when to stop classifying (especially in video) and can produce unstable results. In the end, hey are also probabilistic models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vIci3C4JkL0?start=50\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings; \n",
    "warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vIci3C4JkL0?start=50\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You need to carefully choose your training classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "backimage": "../mybackimage.png",
   "enable_chalkboard": true,
   "scroll": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
