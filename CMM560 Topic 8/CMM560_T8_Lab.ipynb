{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 8 Laboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, you will use imbalanced datasets from the [Keel](https://sci2s.ugr.es/keel/imbalanced.php) public repository to appropriately evaluate performance metric in two popular classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation in Binary Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and making sense of the necessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [glass](https://sci2s.ugr.es/keel/dataset/data/imbalanced/glass-names.txt) dataset. This one has 214 samples of 7 different types of glasses. Each glass sample has 9 features, each corresponding to a different element that composes a glass sample: Rl(??), Na, Mg, Al, Si, K, Ca, Ba and Fe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will start with binary datasets, we will first use the [glass0](https://sci2s.ugr.es/keel/keel-dataset/datasets/imbalanced/imb_IRlowerThan9/names/glass0-names.txt) version of the dataset. The only difference of this one with respect of the original one is that a certain glass class called `class 0` is compared against the rest of the glass classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://sci2s.ugr.es/keel/dataset.php?cod=141) to visit the *glass0* description website, go to the bottom (where it says **Files and additional references** and download the **complete data set**. Unzip the file and store it **in the same directory as this jupyter notebook**. You should get a file with the extension *.dat*. OR, you can simply download `glass0.dat` from Moodle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the data from the file into Python, to do so, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('glass0.dat',\n",
    "                     usecols=range(9), # this only brings the first nine columns of the dataset\n",
    "                     skip_header=14, # The first 14 lines of the .dat contain a description\n",
    "                     delimiter=',')\n",
    "\n",
    "\n",
    "target_names = np.genfromtxt('glass0.dat',\n",
    "                     usecols=range(9,10), # This brings the last column of the dataset, which has the class\n",
    "                     dtype = None,\n",
    "                     encoding = None, # This helps us get the strings in a numpy array\n",
    "                     skip_header=14,\n",
    "                     delimiter=',')\n",
    "\n",
    "print(data,data.shape)\n",
    "print(target_names,target_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that once again, we will store the data and the target in separate variables. Moreover, we will generate a new target variable called `target` with `positive=0` and `negative=1` using the **list comprehension** technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "target = [0 if i == ' positive' else 1 for i in target_names]\n",
    "target=np.array(target)\n",
    "print(target,target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we chose 0 to be the positive and 1 to be the negative since the [glass0 documentation](https://sci2s.ugr.es/keel/dataset.php?cod=141) said so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train a supervised learning model to evaluate. In this case we will use two popular classification models so that we can compare which is better for this dataset: **Support Vector Machine (SVM)** vs. **Random Forests (RF)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split our dataset into training and testing sets (80% train, 20% test). You have already done this in the past!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to split your dataset into training and testing w/ stratification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a SVM model called `model_svm` using the training data and predict the test data. Store the fitted model in a variable called `clf_svm` and the prediction results in a variable called `y_svm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to train a SVM model to predict the labels of the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, train a RF model called `model_rf` (using the `RandomForestClassifier()` function contained in `sklearn.ensemble` package) on the same training data and predict the test data. Store the results in variables called `clf_rf` and `y_rf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to train a RF model to predict the labels of the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `y_svm` and `y_rf` should be *numpy* array vectors of size (43,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In week 3 lab, we saw that once that we predict the values for the test set, we can compare both the `y_test` vector with the predicted vectors (in our case `y_svm` and `y_rf`) to obtain the results. If you recall, it was quite tricky to do this as we had more classes in the vector! Therefore, this time we will do it slightly different..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should know that everything in Python can be imported! Performance metrics can be imported as well. We will first use accuracy (although it should be very clear by now that this one is not suitable for imbalanced datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of SVM: ', accuracy_score(y_svm, y_test))\n",
    "print('Accuracy of RF: ', accuracy_score(y_rf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which is the best classifier in terms of accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC-ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's lecture we saw that one of the most robust metrics for accuracy in the *imbalanced* world is the area under the receiver operating characteristic curve (or AUC-ROC for short). To get this metric, we need to retrain our models, this time in a probabilistic way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that by setting the input `probability=True` when generating our SVM model (RF doesn't need it) and by changing `predict` to `predict_proba`, we will not obtain a fixed output, but instead a probability (value between 0 and 1) of the classification to be 0 or 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the previous models/classes/predictions in our workspace, train new (probabilistic) models in the following cell and store the trained models as `model_svm_proba` and `model_rf_proba`, the classifiers as `clf_svm_proba` and `clf_rf_proba`, and the outputs as `y_svm_proba` and `y_rf_proba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to re-train the models in a probabilistic way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `y_svm_proba` and `y_rf_proba` you should get *numpy* arrays of shape (43, 2), as now each output contains the probability of each sample to be 0 (first value) or 1 (second value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the AUC-ROC. We will **not** get the plot (as we don't have different thresholds), but rather we will obtain the numeric area under the curve (a value between 0 and 1, the larger the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain this values, you can run the following cell (provided that you have correctly calculated `y_svm_proba` and `y_rf_proba` in the previous step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing AUC-ROC considering the probabilities of the positive class (i.e. 0)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# First, we need to extract only the probabilities of classifying 0 (second column of our numpy arrays)\n",
    "# We can do this once again by means of list comprehension\n",
    "y_svm_proba_0 = [p[0] for p in y_svm_proba]\n",
    "y_rf_proba_0 = [p[0] for p in y_rf_proba]\n",
    "\n",
    "print('AUC for SVM: ',roc_auc_score(y_test, y_svm_proba_0))\n",
    "print('AUC for RF: ',roc_auc_score(y_test, y_rf_proba_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting AUC-ROC values **smaller than 0.5** then you can simply invert them (or get the AUC-ROC for the rest/negative class) to get the actual AUC-ROC value for this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comprehensive explanation of why AUC-ROC cannot be smaller than 0.5 can be found [here](https://www.datascienceblog.net/post/machine-learning/interpreting-roc-curves-auc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to test AUC-ROC considering the probabilities of the rest/negative class (i.e. 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the classifier that was better in accuracy still better in AUC?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the previous results were not product of chance or a lucky data split! To do so, we will use cross validation with $k=5$ folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `KFold()`function contained in the `sklearn.model_selection` package you can split a dataset (in this case, the original one contained in the `data` variable) by using the `get_n_splits()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(data)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far you only get a `KFold` object. However, we can put this object inside a for loop to create different training and testing sets in each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(data):\n",
    "    print('\\nFold '+str(i)+':')\n",
    "    print('TRAIN INDEXES:', train_index)\n",
    "    print('TEST INDEXES:', test_index)\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that what is being printed in the cell above are not the samples, but the indexes from where the samples will be taken in each fold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the cell below to implement the **stratified** version of KFolds. Remember that stratification will ensure that both classes are equally present in each set for every fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to do a KFolds split in a stratified way\n",
    "# Hint: Use StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate models using cross validation you **don't** need all of this! In fact, you only use this if you want to store your folds or if you want to implement more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a matter of fact it is easier to simply import the `cross_validate` function from the `sklearn.model_selection` package and get the scores as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validatig the original data (the function will do it all)\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores_svm = cross_validate(model_svm, data, target, cv=5)\n",
    "print('SVM cross-validated scores: ', scores_svm)\n",
    "scores_rf = cross_validate(model_rf, data, target, cv=5)\n",
    "print('RF cross-validated scores: ', scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From these metrics do you notice any disadvantage of RF?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, these are not the only metrics you can get from cross validation! Using the `cross_val_scores` function from the `sklearn.model_selection` package you can get (almost) all of the measures that we discussed in class. You can check the [scoring documentation](https://scikit-learn.org/stable/modules/model_evaluation.html) to see what you can calculate,and below you will find some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import cross_val_scores\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Accuracy for each fold\n",
    "print('Accuracy for SVM: ',cross_val_score(model_svm, data, target, cv=5, scoring = 'accuracy'))\n",
    "print('Accuracy for RF: ',cross_val_score(model_rf, data, target, cv=5, scoring = 'accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Mean Accuracy for all folds\n",
    "print('Mean Accuracy for SVM: ',np.mean(cross_val_score(model_svm, data, target, cv=5, scoring = 'accuracy')))\n",
    "print('Mean Accuracy for RF: ',np.mean(cross_val_score(model_rf, data, target, cv=5, scoring = 'accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Precision, Recall and F1-score (both by fold and average)\n",
    "print('Precision for SVM: ',cross_val_score(model_svm, data, target, cv=5, scoring = 'precision'))\n",
    "print('Precision for RF: ',cross_val_score(model_rf, data, target, cv=5, scoring = 'precision'))\n",
    "print('Recall for SVM: ',cross_val_score(model_svm, data, target, cv=5, scoring = 'recall'))\n",
    "print('Recall for RF: ',cross_val_score(model_rf, data, target, cv=5, scoring = 'recall'))\n",
    "print('F1-Score for SVM: ',cross_val_score(model_svm, data, target, cv=5, scoring = 'f1'))\n",
    "print('F1-Score for RF: ',cross_val_score(model_rf, data, target, cv=5, scoring = 'f1'))\n",
    "\n",
    "print('Mean Precision for SVM: ',np.mean(cross_val_score(model_svm, data, target, cv=5, scoring = 'precision')))\n",
    "print('Mean Precision for RF: ',np.mean(cross_val_score(model_rf, data, target, cv=5, scoring = 'precision')))\n",
    "print('Mean Recall for SVM: ',np.mean(cross_val_score(model_svm, data, target, cv=5, scoring = 'recall')))\n",
    "print('Mean Recall for RF: ',np.mean(cross_val_score(model_rf, data, target, cv=5, scoring = 'recall')))\n",
    "print('Mean F1-Score for SVM: ',np.mean(cross_val_score(model_svm, data, target, cv=5, scoring = 'f1')))\n",
    "print('Mean F1-Score for RF: ',np.mean(cross_val_score(model_rf, data, target, cv=5, scoring = 'f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the Keel repository, there is already a version of the glass dataset called **5FCV**. This means that the authors of this repository already provide a \"proper\" partition of this dataset to test cross validation. If you download this version, you will get 10 *.dat* files instead of 1. **Do you know why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "Run the tests using the 5FCV version of the dataset and compare the results"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
